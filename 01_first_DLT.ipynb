{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0fad0fc0-0b2b-42b9-b668-3b1f33836bc2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### DLT Works with Three types of Datasets.\n",
    "#### Streaming Tables (Temporary/Permananet) - Used as Append Data Sources, Incremental Data.\n",
    "#### Materialized Views - Used for Transformations, Aggregations or Computations.\n",
    "#### Views - Used for Intermediate Transformations, not stored in Target Schema.\n",
    "\n",
    "- DLT pipelines are powered by Delta Lake\n",
    "- We can't use All Purpose Compute to trigger our DLT pipelines, requires Job Compute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f254ea26-d931-4699-9d0d-4d1c2cc7a49d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dlt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a4b4fc44-2b8e-4adb-9bfb-871f6827e587",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a streaming table for Orders\n",
    "\n",
    "@dlt.table(\n",
    "    table_properties = {\"quality\": \"bronze\"},\n",
    "    comment = \"Orders Bronze Table\"\n",
    ")\n",
    "def orders_bronze(): # by default the function name is the table name.\n",
    "    # need a streaming source to create a streaming table.\n",
    "    # delta tables have a property to work as input source for both streaming and batch modes.\n",
    "    df = spark.readStream.table(\"syk_poc.`test-pranav`.orders_raw\")\n",
    "\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7da6c697-9dbf-479d-96df-fcb58726a372",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a Materialized View for Customers\n",
    "\n",
    "@dlt.table(\n",
    "    table_properties = {\"quality\": \"bronze\"},\n",
    "    comment = \"Customer Bronze Table\",\n",
    "    name = \"customer_bronze\"\n",
    ")\n",
    "def cust():\n",
    "    # for a Materialized View the input source is a batch, not a stream.\n",
    "    df = spark.read.table(\"syk_poc.`test-pranav`.customer_raw\")\n",
    "\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8cf658ef-9e23-43ec-8b35-dc344bdefae5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a View to join orders with Customers\n",
    "\n",
    "# DLT Views are temporary and are used for intermediate transformation, they are not registered as targets.\n",
    "# We use the LIVE Keyword to reference the DLT tables and views created in the same pipeline.\n",
    "\n",
    "# @dlt.table -> Streaming table and Materialized Views\n",
    "# @dlt.view -> for DLT Views\n",
    "\n",
    "@dlt.view(\n",
    "    comment = \"Joined View\",\n",
    ")\n",
    "def joined_vw():\n",
    "    # for a Materialized View the input source is a batch, not a stream.\n",
    "    df_c = spark.read.table(\"LIVE.customer_bronze\")\n",
    "    df_o = spark.read.table(\"LIVE.orders_bronze\")\n",
    "\n",
    "    df_join = df_o.join(df_c, how = \"left_outer\", on = df_c.c_custkey == df_o.o_custkey)\n",
    "\n",
    "    return df_join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2fd7c500-949f-4646-912d-3188bc7534cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a Materialized View to add a new column\n",
    "\n",
    "from pyspark.sql.functions import current_timestamp\n",
    "\n",
    "@dlt.table(\n",
    "    table_properties = {\"quality\": \"silver\"},\n",
    "    comment = \"Joined Table\",\n",
    "    name = \"joined_silver\"\n",
    ")\n",
    "def joined_silver():\n",
    "    # for a Materialized View the input source is a batch, not a stream.\n",
    "    df = spark.read.table(\"LIVE.joined_vw\").withColumn(\"__insert_date\", current_timestamp())\n",
    "\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f6c1a801-aba0-4a26-b3f6-b7167c42a251",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Aggregate based on c_mktsegment and find the count of order (o_orderkey)\n",
    "\n",
    "from pyspark.sql.functions import count\n",
    "\n",
    "@dlt.table(\n",
    "    table_properties = {\"quality\": \"gold\"},\n",
    "    comment = \"Aggregated Orders Table\"\n",
    ")\n",
    "def orders_agg_gold():\n",
    "    df = spark.read.table(\"LIVE.joined_silver\")\n",
    "    \n",
    "    df_final = df.groupBy(\"c_mktsegment\").agg(count(\"o_orderkey\").alias(\"sum_orders\")).withColumn(\"__insert_date\", current_timestamp())\n",
    "\n",
    "    return df_final "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6383144667734579,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "01_first_DLT",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}